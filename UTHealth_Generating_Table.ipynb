{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34943a8",
   "metadata": {},
   "source": [
    "# This program was written by Angelica Sharma\n",
    "\n",
    "## Goal 1: Create a table with normalized weight scores for gene pairs available in our data set, taking into consideration 7 factors:\n",
    "\n",
    "1. Co-Expression\n",
    "2. Biological Pathway\n",
    "3. Protein-Protein Interactions\n",
    "4. Cellular Co-localizations\n",
    "5. Genetic Interaction \n",
    "6. Shared Protein Domain\n",
    "7. Gene Regularatory Interaction Network\n",
    "\n",
    "We will then use these factors to a create one, final normalized score for each gene pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13508b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getUrlsFromGeneMania(name):\n",
    "    url = 'http://genemania.org/data/current/Homo_sapiens/'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    link_tags = soup.find_all('a')\n",
    "    urls = []\n",
    "    for link in link_tags:\n",
    "        href = link.get('href')\n",
    "        if href.startswith(name):\n",
    "            url = \"{}{}\".format('http://genemania.org/data/current/Homo_sapiens/', href)\n",
    "            urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d188519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datatable as dt\n",
    "import urllib.request\n",
    "from collections import defaultdict\n",
    "\n",
    "def readData(urls):\n",
    "    # Create a defaultdict to store gene pairs and weights\n",
    "    gene_pairs = defaultdict(float)\n",
    "\n",
    "    # Iterate through the files containing gene pairs and weights\n",
    "    for file in urls:\n",
    "        # Read the contents of the file into a list of lines\n",
    "        webUrl = urllib.request.urlopen(file)\n",
    "    \n",
    "        # Skip the first line\n",
    "        data = webUrl.readlines()[1:]\n",
    "    \n",
    "        # Iterate through the lines and update the defaultdict\n",
    "        for line in data:\n",
    "            values = line.decode().strip().split(\"\\t\")\n",
    "            gene1, gene2 = values[0], values[1]\n",
    "            weight = float(values[2])\n",
    "        \n",
    "            # Normalize the gene pair representation\n",
    "            gene_pair = f\"{min(gene1, gene2)},{max(gene1, gene2)}\"\n",
    "        \n",
    "            # Add the weight to the existing value in the defaultdict\n",
    "            gene_pairs[gene_pair] += weight\n",
    "            \n",
    "        # To keep track of progress\n",
    "        print('Finished analyzing ' + file)\n",
    "        \n",
    "    # Create a datatable from the defaultdict - more efficient!\n",
    "    table = dt.Frame(GenePair=list(gene_pairs.keys()), Weight=list(gene_pairs.values()))\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc810074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatable import update, f\n",
    "def normalizeWeights(tab, name):\n",
    "    mean = tab[:, dt.mean(dt.f[1])][0, 0]\n",
    "    print(\"Mean:\", mean)\n",
    "    std = tab[:, dt.sd(dt.f[1])][0, 0]\n",
    "    print(\"Standard Deviation:\", std)\n",
    "\n",
    "    # Normalize the values in column 2 using mean and standard deviation\n",
    "    tab[:, name] = (f[name] - mean) / std\n",
    "    return tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5198f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeFactor(urlName, tableName):\n",
    "    \n",
    "    dt.options.display.max_nrows = None\n",
    "    #Get list of files from genemania\n",
    "    urls = getUrlsFromGeneMania(urlName)\n",
    "    \n",
    "    #Read files and create a table with weights\n",
    "    tab = readData(urls)\n",
    "    print(tab)\n",
    "    \n",
    "    #Rename second column for readability\n",
    "    tab.names = {'Weight': tableName}\n",
    "    \n",
    "    #Normalize the weights in the table\n",
    "    tab = normalizeWeights(tab, tableName)\n",
    "    print(tab)\n",
    "    \n",
    "    return tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c91ab1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def analyzeTxtFiles(fileList, tableName):\n",
    "    \n",
    "    # Create a defaultdict to store gene pairs and weights\n",
    "    gene_pairs = defaultdict(float)\n",
    "    for file in fileList:\n",
    "        with open(file, \"r\") as data:\n",
    "            # Skip the first line\n",
    "            next(data)  \n",
    "            for line in data:\n",
    "    \n",
    "                # Iterate through the lines and update the defaultdict\n",
    "                values = line.strip().split(\"\\t\")\n",
    "                gene1, gene2 = str(values[0]), str(values[1])\n",
    "                \n",
    "                # Skip over gene pairs that don't have any weight data\n",
    "                if(values[4] != ''):\n",
    "                    weight = float(values[4])\n",
    "        \n",
    "                # Normalize the gene pair representation, and make sure they are all uppercase letters\n",
    "                gene_pair = f\"{min(gene1, gene2).upper()},{max(gene1, gene2).upper()}\"\n",
    "        \n",
    "                # Add the weight to the existing value in the defaultdict\n",
    "                gene_pairs[gene_pair] += weight\n",
    "            \n",
    "            # To keep track of progress\n",
    "            print('Finished analyzing ' + file)\n",
    "        \n",
    "    # Create a datatable from the defaultdict - more efficient!\n",
    "    tab = dt.Frame(GenePair=list(gene_pairs.keys()), Weight=list(gene_pairs.values()))\n",
    "    \n",
    "    #Rename second column for readability\n",
    "    tab.names = {'Weight': tableName}\n",
    "    \n",
    "    #Normalize the weights in the table\n",
    "    tab = normalizeWeights(tab, tableName)\n",
    "    \n",
    "    return tab\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf9fee",
   "metadata": {},
   "source": [
    "Creating tables for each factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Co-expression - 1\n",
    "\n",
    "co_exp_table = analyzeFactor('Co-expression.', 'Co-expression Weight')\n",
    "%store co_exp_table\n",
    "print(co_exp_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pathway - 2\n",
    "\n",
    "pathway_table = analyzeFactor('Pathway.', 'Pathway Weight')\n",
    "%store pathway_table\n",
    "print(pathway_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef43535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Protein-Protein Interactions - 3\n",
    "\n",
    "protein_table = analyzeFactor('Physical_Interactions.', 'Protein-Protein Weight')\n",
    "%store protein_table\n",
    "print(protein_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ecb082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cellular Localizations - 4\n",
    "\n",
    "localization_table = analyzeFactor('Co-localization.', 'Co-localization Weight')\n",
    "%store localization_table\n",
    "print(localization_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7fbcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genetic Interactions - 5\n",
    "\n",
    "genetic_interaction_table = analyzeFactor('Genetic_Interactions.', 'Genetic Interaction Weight')\n",
    "%store genetic_interaction_table\n",
    "print(genetic_interaction_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc575342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shared Protein Domains - 6\n",
    "\n",
    "shared_protein_table = analyzeFactor('Shared_protein_domains.', 'Shared Protein Domain Weight')\n",
    "%store shared_protein_table\n",
    "print(shared_protein_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a958fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gene Regularatory Interaction Network - has standard gene symbol names - 7\n",
    "\n",
    "fileList = ['Adult-Heart-regulons.txt', 'Heart_GTEx-regulons.txt', 'Fetal-Heart-regulons.txt', 'whole_NeonatalHeart-regulons.txt']\n",
    "gene_reg_table = analyzeTxtFiles(fileList, 'Gene Regularatory Weight')\n",
    "%store gene_reg_table\n",
    "print(gene_reg_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d2e4e",
   "metadata": {},
   "source": [
    "Combining tables 1-6 to form one, large table. Excluding table 7 for now, since table 7 is in gene symbols while the rest are in Uniprot IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d97fdb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datatable as dt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to add/update weights in the big table\n",
    "def update_weights(gene_pair, weight, big_table_dict):\n",
    "    big_table_dict[gene_pair].append(weight)\n",
    "\n",
    "def merge_and_update(table, colName, big_table_dict):\n",
    "     for i in range(table.nrows):\n",
    "        gene_pair = table[i, \"GenePair\"].upper()\n",
    "        weight = table[i, colName]\n",
    "        update_weights(gene_pair, weight, big_table_dict)\n",
    "        if(i % 1_000_000 == 0):\n",
    "            print(\"Finished \" + str(i) + \"/\" + str(table.nrows))\n",
    "     print(\"Finished adding table with \" + colName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571f125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tables 1-6\n",
    "%store -r co_exp_table\n",
    "%store -r pathway_table\n",
    "%store -r protein_table\n",
    "%store -r localization_table\n",
    "%store -r genetic_interaction_table\n",
    "%store -r shared_protein_table\n",
    "\n",
    "# Create the initial big table as a dictionary\n",
    "big_table_dict = defaultdict(list)\n",
    "\n",
    "# Merge and update weights for each table\n",
    "merge_and_update(shared_protein_table, \"Shared Protein Domain Weight\", big_table_dict)\n",
    "merge_and_update(genetic_interaction_table, \"Genetic Interaction Weight\", big_table_dict)\n",
    "merge_and_update(localization_table, \"Co-localization Weight\", big_table_dict)\n",
    "merge_and_update(protein_table, \"Protein-Protein Weight\", big_table_dict)\n",
    "merge_and_update(pathway_table, \"Pathway Weight\", big_table_dict)\n",
    "merge_and_update(co_exp_table, \"Co-expression Weight\", big_table_dict)\n",
    "\n",
    "print(\"Finished adding all tables. Converting dictionary to dataframe...\")\n",
    "\n",
    "# Create empty lists for GenePair and WeightList columns\n",
    "gene_pairs = []\n",
    "weight_lists = []\n",
    "\n",
    "# Iterate over the dictionary and populate the lists\n",
    "for gene_pair, weight_list in big_table_dict.items():\n",
    "    gene_pairs.append(gene_pair)\n",
    "    weight_lists.append(weight_list)\n",
    "\n",
    "# Create the big table using the populated lists\n",
    "big_table = dt.Frame(GenePair=gene_pairs, WeightList=weight_lists)\n",
    "\n",
    "print(big_table)\n",
    "%store big_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a2ae5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         | GenePair       WeightList                                      \n",
      "         | str32          arr32(float64)                                  \n",
      "-------- + -------------  ------------------------------------------------\n",
      "       0 | Q16850,Q9NR63  [0.280541, -0.138305]                           \n",
      "       1 | O15519,Q92851  [2.90248, 70.4736, 2.58423, -0.0948521, 2.32987]\n",
      "       2 | Q13275,Q9Y4D7  [0.225917, 0.249665]                            \n",
      "       3 | P52756,P78332  [0.690218, 0.107048, 33.7433]                   \n",
      "       4 | P13569,Q2M3G0  [0.376132]                                      \n",
      "       5 | P30260,Q02790  [0.103014, 0.249635]                            \n",
      "       6 | P05141,Q9UJS0  [0.198605, 0.0245982]                           \n",
      "       7 | P46063,Q9H6R0  [-0.156449]                                     \n",
      "       8 | P31270,P50221  [-0.00623345, -0.0246768]                       \n",
      "       9 | Q02790,Q9H6T3  [0.089358, -0.533407]                           \n",
      "      10 | P30260,Q9H6T3  [0.321509, -0.0632663, 2.56159]                 \n",
      "      11 | Q6FI81,Q7L592  [-0.156449]                                     \n",
      "      12 | P13569,P21439  [0.376132]                                      \n",
      "      13 | P21439,Q2M3G0  [0.376132]                                      \n",
      "      14 | Q96Q42,Q9P2D0  [0.11667, -0.514763]                            \n",
      "       … | …              …                                               \n",
      "46896929 | P48047,Q96KJ9  [-0.627962]                                     \n",
      "46896930 | Q2M3X9,Q9H7X3  [-0.580019]                                     \n",
      "46896931 | Q5TEC3,Q9UK10  [-0.548057]                                     \n",
      "46896932 | P51504,Q9GZX5  [-0.594668]                                     \n",
      "46896933 | P48742,Q2M218  [-0.553384]                                     \n",
      "[46896934 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%store -r big_table\n",
    "print(big_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e3249",
   "metadata": {},
   "source": [
    "Extracting the unique genes in this large table & creating a dictionary to convert them to gene symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def convert_uniprot_to_gene(uniprot_id):\n",
    "    url = f\"https://www.uniprot.org/uniprot/{uniprot_id}.xml\"\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        xml_data = response.text\n",
    "        root = ET.fromstring(xml_data)\n",
    "        gene_symbol_element = root.find(\".//{http://uniprot.org/uniprot}gene/{http://uniprot.org/uniprot}name[@type='primary']\")\n",
    "        if gene_symbol_element is not None:\n",
    "            gene_symbol = gene_symbol_element.text\n",
    "            return gene_symbol\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def convert_ensembl_to_gene_symbol(ensembl_id):\n",
    "    url = f\"https://rest.ensembl.org/lookup/id/{ensembl_id}\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.ok:\n",
    "        data = response.json()\n",
    "        if \"display_name\" in data:\n",
    "            gene_symbol = data[\"display_name\"]\n",
    "            return gene_symbol\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec712471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathering unique genes in the combined table\n",
    "\n",
    "%store -r big_table\n",
    "\n",
    "unique_genes = set()\n",
    "\n",
    "gene_pairs_list = big_table['GenePair'].to_list()[0]\n",
    "\n",
    "# Iterate over the gene pairs list\n",
    "for gene_pair in gene_pairs_list:\n",
    "    \n",
    "    # Split the gene pair by the delimiter \n",
    "    genes = gene_pair.split(',') \n",
    "    \n",
    "    # Add each gene to set\n",
    "    for gene in genes:\n",
    "        unique_genes.add(gene.strip())\n",
    "        \n",
    "print(\"We have \" + str(len(unique_genes)) + \" distinct genes.\")\n",
    "print(unique_genes)\n",
    "%store unique_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7444ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary that maps each UniprotID/Ensembl ID to its gene symbol, if one exists. \n",
    "#Using multiprocessing to make this faster using Dask.\n",
    "\n",
    "import dask\n",
    "import dask.distributed\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "#Creating a dictionary that maps each UniprotID to its gene symbol. \n",
    "#This allows multiple processes to access and update the dictionary concurrently.\n",
    "gene_to_symbol = dict()\n",
    "%store -r unique_genes\n",
    "gene_list = list(unique_genes)\n",
    "\n",
    "for gene in gene_list:\n",
    "    gene_to_symbol[gene] = None\n",
    "\n",
    "#This ensures that the API call for each gene is executed lazily and in a parallelizable manner.\n",
    "@dask.delayed\n",
    "def process_gene(gene):\n",
    "    original_gene = gene.strip()\n",
    "\n",
    "    # check if uniprot\n",
    "    output = convert_uniprot_to_gene(str(original_gene))\n",
    "\n",
    "    # otherwise check if ensembl\n",
    "    if output is None:\n",
    "        output = convert_ensembl_to_gene_symbol(str(original_gene))\n",
    "        \n",
    "    #if we still couldn't find a gene symbol, we will keep it the same\n",
    "    if output is None:\n",
    "        output = original_gene\n",
    "    gene_symbol = output\n",
    "\n",
    "    return original_gene, gene_symbol\n",
    "\n",
    "i = 0\n",
    "length = len(gene_list)\n",
    "genes_per_process = 547  # Number of genes to process per process\n",
    "\n",
    "# Split the gene pairs into chunks for parallel processing\n",
    "chunks = [gene_list[i:i + genes_per_process] for i in range(0, length, genes_per_process)]\n",
    "\n",
    "# Create a Dask client\n",
    "client = dask.distributed.Client()\n",
    "\n",
    "# Scatter the gene_to_symbol dictionary to all workers\n",
    "gene_to_symbol_future = client.scatter(gene_to_symbol)\n",
    "\n",
    "results = []\n",
    "# Iterate over the chunks and process them in parallel\n",
    "for chunk in chunks:\n",
    "    start_time = time.time()\n",
    "    futures = []\n",
    "    for gene in chunk:\n",
    "        future = process_gene(gene)\n",
    "        futures.append(future)\n",
    "\n",
    "   # Compute the results of the current chunk\n",
    "    chunk_results = dask.compute(futures)\n",
    "    results.extend(chunk_results)\n",
    "    \n",
    "    i += len(chunk)\n",
    "    print(\"Progress: {}/{}\".format(i, length))\n",
    "    \n",
    "    # Print CPU and memory utilization\n",
    "    cpu_percent = psutil.cpu_percent()\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    memory_percent = memory_info.percent\n",
    "    print(\"CPU utilization: {}%\".format(cpu_percent))\n",
    "    print(\"Memory utilization: {}%\".format(memory_percent))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed Time: {:.2f} seconds\".format(elapsed_time))\n",
    "    \n",
    "# Update the gene_to_symbol dictionary with the results\n",
    "for this_chunk in results:\n",
    "    for gene in this_chunk:\n",
    "        gene_to_symbol[gene[0]] = gene[1]\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "\n",
    "print(gene_to_symbol)\n",
    "%store gene_to_symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35413a49",
   "metadata": {},
   "source": [
    "Converting tables 1 - 6 to gene symbols using the created dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6a2fb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting datatable to a list of lists...\n",
      "Done. Splitting data list into chunks...\n",
      "Done. Scattering chunks to Dask workers...\n",
      "Done. Applying update function to each chunk in parallel...\n",
      "Elapsed Time: 4.44 seconds\n",
      "Done. Combining the updated chunks into a single datatable...\n",
      "Converting to csv file...\n"
     ]
    }
   ],
   "source": [
    "%store -r shared_protein_table\n",
    "shared_protein_table = convert_to_gene_symbols(gene_to_symbol, shared_protein_table)\n",
    "shared_protein_table.names = {'C0': 'GenePair'}\n",
    "print(\"Converting to csv file...\")\n",
    "shared_protein_table.to_csv(\"shared_protein_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c76363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting datatable to a list of lists...\n",
      "Done. Splitting data list into chunks...\n",
      "Done. Scattering chunks to Dask workers...\n",
      "Done. Applying update function to each chunk in parallel...\n",
      "Elapsed Time: 29.00 seconds\n",
      "Done. Combining the updated chunks into a single datatable...\n",
      "Converting to csv file...\n"
     ]
    }
   ],
   "source": [
    "%store -r genetic_interaction_table\n",
    "genetic_interaction_table = convert_to_gene_symbols(gene_to_symbol, genetic_interaction_table)\n",
    "genetic_interaction_table.names = {'C0': 'GenePair'}\n",
    "print(\"Converting to csv file...\")\n",
    "genetic_interaction_table.to_csv(\"genetic_interaction_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1058ac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting datatable to a list of lists...\n",
      "Done. Splitting data list into chunks...\n",
      "Done. Scattering chunks to Dask workers...\n",
      "Done. Applying update function to each chunk in parallel...\n",
      "Elapsed Time: 2.60 seconds\n",
      "Done. Combining the updated chunks into a single datatable...\n",
      "Converting to csv file...\n"
     ]
    }
   ],
   "source": [
    "%store -r localization_table\n",
    "localization_table = convert_to_gene_symbols(gene_to_symbol, localization_table)\n",
    "localization_table.names = {'C0': 'GenePair'}\n",
    "print(\"Converting to csv file...\")\n",
    "localization_table.to_csv(\"localization_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r protein_table\n",
    "print(protein_table)\n",
    "protein_table = convert_to_gene_symbols(gene_to_symbol, protein_table)\n",
    "protein_table.names = {'C0': 'GenePair'}\n",
    "print(\"Converting to csv file...\")\n",
    "protein_table.to_csv(\"protein_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e57f4143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting datatable to a list of lists...\n",
      "Done. Splitting data list into chunks...\n",
      "Done. Scattering chunks to Dask workers...\n",
      "Done. Applying update function to each chunk in parallel...\n",
      "Elapsed Time: 0.50 seconds\n",
      "Done. Combining the updated chunks into a single datatable...\n",
      "Converting to csv file...\n"
     ]
    }
   ],
   "source": [
    "%store -r pathway_table\n",
    "pathway_table = convert_to_gene_symbols(gene_to_symbol, pathway_table)\n",
    "pathway_table.names = {'C0': 'GenePair'}\n",
    "print(\"Converting to csv file...\")\n",
    "pathway_table.to_csv(\"pathway_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc45cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r co_exp_table\n",
    "print(co_exp_table)\n",
    "co_exp_table = convert_to_gene_symbols(gene_to_symbol, co_exp_table)\n",
    "co_exp_table.names = {'C0': 'GenePair'}\n",
    "print(co_exp_table)\n",
    "print(\"Converting to csv file...\")\n",
    "co_exp_table.to_csv(\"co_exp_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250fb709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also storing table 7, which is already in gene symbols\n",
    "\n",
    "%store -r gene_reg_table\n",
    "print(\"Storing gene_reg_table in csv...\")\n",
    "gene_reg_table.to_csv('gene_reg_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57587bd8",
   "metadata": {},
   "source": [
    "Combining all gene pairs from all 7 tables into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8713e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datatable as dt\n",
    "\n",
    "def merge_to_final_table(table, combined_table):\n",
    "    gene_pairs = table[:, 0]\n",
    "    combined_table.rbind(gene_pairs)\n",
    "    return combined_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8bdcb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging shared_protein_table...\n",
      "Done. Merging genetic_interaction_table...\n",
      "Done. Merging localization_table...\n",
      "Done. Merging protein_table...\n",
      "Done. Merging pathway_table...\n",
      "Done. Merging co_exp_table...\n",
      "Done. Merging gene_reg_table...\n",
      "         | GenePair        \n",
      "         | str32           \n",
      "-------- + ----------------\n",
      "       0 | CYP51A1,CYP26B1 \n",
      "       1 | CFLAR,CASP10    \n",
      "       2 | SEMA3F,PLXND1   \n",
      "       3 | RBM5,RBM6       \n",
      "       4 | CFTR,ABCB5      \n",
      "       5 | CDC27,FKBP4     \n",
      "       6 | SLC25A5,SLC25A13\n",
      "       7 | RECQL,DHX33     \n",
      "       8 | HOXA11,MEOX1    \n",
      "       9 | FKBP4,RPAP3     \n",
      "      10 | CDC27,RPAP3     \n",
      "      11 | CIAPIN1,NDUFAF7 \n",
      "      12 | CFTR,ABCB4      \n",
      "      13 | ABCB4,ABCB5     \n",
      "      14 | ALS2,IBTK       \n",
      "       … | …               \n",
      "50538058 | PLA2G12A,ZMIZ1  \n",
      "50538059 | PRKRA,ZMIZ1     \n",
      "50538060 | STAM,ZMIZ1      \n",
      "50538061 | TRF,ZMIZ1       \n",
      "50538062 | WDR41,ZMIZ1     \n",
      "[50538063 rows x 1 column]\n",
      "\n",
      "Done. Deleting duplicate pairs...\n",
      "Done. Converting to csv file...\n",
      "         | GenePair            \n",
      "         | str32               \n",
      "-------- + --------------------\n",
      "       0 | 0610007P14RIK,ELK3  \n",
      "       1 | 0610007P14RIK,RAD21 \n",
      "       2 | 0610007P14RIK,TAF1  \n",
      "       3 | 0610007P14RIK,USF2  \n",
      "       4 | 0610009B22RIK,KDM5A \n",
      "       5 | 0610009B22RIK,TAF7  \n",
      "       6 | 0610009L18RIK,FOXP1 \n",
      "       7 | 0610009O20RIK,GTF2F1\n",
      "       8 | 0610030E20RIK,BRF2  \n",
      "       9 | 0610030E20RIK,ETS2  \n",
      "      10 | 1010001N08RIK,BRF2  \n",
      "      11 | 1010001N08RIK,NFYC  \n",
      "      12 | 1010001N08RIK,USF1  \n",
      "      13 | 1110004E09RIK,ATF3  \n",
      "      14 | 1110004E09RIK,KLF4  \n",
      "       … | …                   \n",
      "47100707 | ZZZ3,ZSWIM3         \n",
      "47100708 | ZZZ3,ZSWIM5         \n",
      "47100709 | ZZZ3,ZSWIM6         \n",
      "47100710 | ZZZ3,ZUP1           \n",
      "47100711 | ZZZ3,ZYG11B         \n",
      "[47100712 rows x 1 column]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datatable as dt\n",
    "\n",
    "combined_table = dt.Frame()\n",
    "print(\"Merging shared_protein_table...\")\n",
    "combined_table = merge_to_final_table(shared_protein_table, combined_table)\n",
    "print(\"Done. Merging genetic_interaction_table...\")\n",
    "combined_table = merge_to_final_table(genetic_interaction_table, combined_table)\n",
    "print(\"Done. Merging localization_table...\")\n",
    "combined_table = merge_to_final_table(localization_table, combined_table)\n",
    "print(\"Done. Merging protein_table...\")\n",
    "combined_table = merge_to_final_table(protein_table, combined_table)\n",
    "print(\"Done. Merging pathway_table...\")\n",
    "combined_table = merge_to_final_table(pathway_table, combined_table)\n",
    "print(\"Done. Merging co_exp_table...\")\n",
    "combined_table = merge_to_final_table(co_exp_table, combined_table)\n",
    "print(\"Done. Merging gene_reg_table...\")\n",
    "combined_table = merge_to_final_table(gene_reg_table, combined_table)\n",
    "print(combined_table)\n",
    "print(\"Done. Deleting duplicate pairs...\")\n",
    "combined_table = dt.unique(combined_table['GenePair'])\n",
    "print(\"Done. Converting to csv file...\")\n",
    "combined_table.to_csv(\"combined_table_gene_pair_symbols.csv\")\n",
    "print(combined_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf532849",
   "metadata": {},
   "source": [
    "Creating new & final large table with all factors and all entries filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2cbc58b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading genepairs from large table into a dask dataframe...\n",
      "Loading table 7 as dask dataframe...\n",
      "Merging large dataframe with this table..\n",
      "Calculating the average weight from the small dataframe...\n",
      "Done. The average is: 1.9726663895399094e-14\n",
      "Filling in any empty weights in the merged dataframe with the average weight...\n",
      "Done.\n",
      "\n",
      "Loading table 6 as dask dataframe...\n",
      "Merging large dataframe with this table..\n",
      "Calculating the average weight from the small dataframe...\n",
      "Done. The average is: -4.583351362155133e-13\n",
      "Filling in any empty weights in the merged dataframe with the average weight...\n",
      "Done.\n",
      "\n",
      "Loading table 5 as dask dataframe...\n",
      "Merging large dataframe with this table..\n",
      "Calculating the average weight from the small dataframe...\n",
      "Done. The average is: -1.178984198571654e-13\n",
      "Filling in any empty weights in the merged dataframe with the average weight...\n",
      "Done.\n",
      "\n",
      "Loading table 4 as dask dataframe...\n",
      "Merging large dataframe with this table..\n",
      "Calculating the average weight from the small dataframe...\n",
      "Done. The average is: -9.152275754241174e-13\n",
      "Filling in any empty weights in the merged dataframe with the average weight...\n",
      "Done.\n",
      "\n",
      "Loading table 3 as dask dataframe...\n",
      "Merging large dataframe with this table..\n",
      "Calculating the average weight from the small dataframe...\n",
      "Done. The average is: 8.39881277139375e-14\n",
      "Filling in any empty weights in the merged dataframe with the average weight...\n",
      "Done.\n",
      "\n",
      "Loading table 2 as dask dataframe...\n",
      "Merging large dataframe with this table..\n",
      "Calculating the average weight from the small dataframe...\n",
      "Done. The average is: -1.0801328757163808e-13\n",
      "Filling in any empty weights in the merged dataframe with the average weight...\n",
      "Done.\n",
      "\n",
      "Loading table 1 as dask dataframe...\n",
      "Merging large dataframe with this table..\n",
      "Calculating the average weight from the small dataframe...\n",
      "Done. The average is: 2.0123345617195337e-12\n",
      "Filling in any empty weights in the merged dataframe with the average weight...\n",
      "Done.\n",
      "\n",
      "          GenePair  Gene Regularatory Weight  Shared Protein Domain Weight  \\\n",
      "0     MAML1,TUBB2B              1.972666e-14                 -4.583351e-13   \n",
      "1     MAML1,UBE2E2              1.972666e-14                 -4.583351e-13   \n",
      "2     MAML1,ZNF180              1.972666e-14                 -4.583351e-13   \n",
      "3       MAML2,CD93              1.972666e-14                 -4.583351e-13   \n",
      "4      MAML2,FGF13              1.972666e-14                 -4.583351e-13   \n",
      "...            ...                       ...                           ...   \n",
      "9995    MYF6,DDX43              1.972666e-14                 -4.583351e-13   \n",
      "9996    MYF6,ELFN2              1.972666e-14                 -4.583351e-13   \n",
      "9997  MYF6,FAM187B              1.972666e-14                 -4.583351e-13   \n",
      "9998     MYF6,FN3K              1.972666e-14                 -4.583351e-13   \n",
      "9999   MYF6,HOXC13              1.972666e-14                 -4.583351e-13   \n",
      "\n",
      "      Genetic Interaction Weight  Co-localization Weight  \\\n",
      "0                  -1.178984e-13           -9.152276e-13   \n",
      "1                  -1.178984e-13           -9.152276e-13   \n",
      "2                  -1.178984e-13           -9.152276e-13   \n",
      "3                  -1.162937e-01           -9.152276e-13   \n",
      "4                  -1.142927e-01           -9.152276e-13   \n",
      "...                          ...                     ...   \n",
      "9995               -1.178984e-13           -9.152276e-13   \n",
      "9996               -1.178984e-13           -9.152276e-13   \n",
      "9997               -1.178984e-13           -9.152276e-13   \n",
      "9998               -1.178984e-13           -9.152276e-13   \n",
      "9999               -1.178984e-13           -9.152276e-13   \n",
      "\n",
      "      Protein-Protein Weight  Pathway Weight  Co-expression Weight  \n",
      "0               8.398813e-14   -1.080133e-13             -0.193810  \n",
      "1               8.398813e-14   -1.080133e-13              0.031257  \n",
      "2               8.398813e-14   -1.080133e-13              0.385504  \n",
      "3               8.398813e-14   -1.080133e-13             -0.530744  \n",
      "4               8.398813e-14   -1.080133e-13             -0.305677  \n",
      "...                      ...             ...                   ...  \n",
      "9995            8.398813e-14   -1.080133e-13             -0.273715  \n",
      "9996            8.398813e-14   -1.080133e-13             -0.063298  \n",
      "9997            8.398813e-14   -1.080133e-13              0.239011  \n",
      "9998            8.398813e-14   -1.080133e-13              0.015276  \n",
      "9999            8.398813e-14   -1.080133e-13             -0.520090  \n",
      "\n",
      "[10000 rows x 8 columns]\n",
      "Saving the merged dataframe as a csv file...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "def mergeTable(small_table, large_table, columnName):\n",
    "    print(\"Merging large dataframe with this table..\")\n",
    "    merged_df = large_table.merge(small_table, on='GenePair', how='left')\n",
    "    print(\"Calculating the average weight from the small dataframe...\")\n",
    "    avg = small_table[columnName].mean().compute()\n",
    "    print(\"Done. The average is: \" + str(avg))\n",
    "    print(\"Filling in any empty weights in the merged dataframe with the average weight...\")\n",
    "    merged_df[columnName] = merged_df[columnName].fillna(avg)\n",
    "    print('Done.\\n')\n",
    "    return merged_df\n",
    "\n",
    "print(\"Loading genepairs from large table into a dask dataframe...\")\n",
    "large_df = dd.read_csv('combined_table_gene_pair_symbols.csv', header=0, usecols=['GenePair'])\n",
    "\n",
    "#Table 7\n",
    "print(\"Loading table 7 as dask dataframe...\")\n",
    "gene_reg_df = dd.read_csv('gene_reg_table.csv')\n",
    "large_df = mergeTable(gene_reg_df, large_df, 'Gene Regularatory Weight')\n",
    "\n",
    "#Table 6\n",
    "print(\"Loading table 6 as dask dataframe...\")\n",
    "shared_protein_df = dd.read_csv('shared_protein_table.csv')\n",
    "large_df = mergeTable(shared_protein_df, large_df, 'Shared Protein Domain Weight')\n",
    "\n",
    "#Table 5\n",
    "print(\"Loading table 5 as dask dataframe...\")\n",
    "genetic_interaction_df = dd.read_csv('genetic_interaction_table.csv')\n",
    "large_df = mergeTable(genetic_interaction_df, large_df, 'Genetic Interaction Weight')\n",
    "\n",
    "#Table 4\n",
    "print(\"Loading table 4 as dask dataframe...\")\n",
    "localization_df = dd.read_csv('localization_table.csv')\n",
    "large_df = mergeTable(localization_df, large_df, 'Co-localization Weight')\n",
    "\n",
    "#Table 3\n",
    "print(\"Loading table 3 as dask dataframe...\")\n",
    "protein_df = dd.read_csv('protein_table.csv')\n",
    "large_df = mergeTable(protein_df, large_df, 'Protein-Protein Weight')\n",
    "\n",
    "#Table 2\n",
    "print(\"Loading table 2 as dask dataframe...\")\n",
    "pathway_df = dd.read_csv('pathway_table.csv')\n",
    "large_df = mergeTable(pathway_df, large_df, 'Pathway Weight')\n",
    "\n",
    "#Table 1\n",
    "print(\"Loading table 1 as dask dataframe...\")\n",
    "co_exp_df = dd.read_csv('co_exp_table.csv')\n",
    "large_df = mergeTable(co_exp_df, large_df, 'Co-expression Weight')\n",
    "\n",
    "print(large_df.head(n=10000, npartitions=1, compute=True))\n",
    "print(\"Saving the merged dataframe as a csv file...\")\n",
    "large_df.to_csv('final_table_gene_data.csv')\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b989e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca075c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from datatable import dt, update\n",
    "import dask.array as da\n",
    "import time\n",
    "%store -r big_table\n",
    "%store -r gene_to_symbol\n",
    "\n",
    "print(\"Converting datatable to a list of lists...\")\n",
    "# Convert the datatable to a list of lists\n",
    "data_list = big_table[:, 0].to_list()\n",
    "\n",
    "print(\"Done. Splitting data list into chunks...\")\n",
    "# Split the data list into chunks\n",
    "chunks = dask.delayed(data_list)\n",
    "\n",
    "\n",
    "# Define the function to update the gene symbols\n",
    "def update_gene_symbol(gene):\n",
    "    gene1, gene2 = gene.split(',')[0], gene.split(',')[1]\n",
    "    output = gene_to_symbol.get(gene1, gene1) + \",\" + gene_to_symbol.get(gene2, gene2)\n",
    "    return output\n",
    "\n",
    "# Define the function to update a chunk\n",
    "def update_chunk(chunk):\n",
    "    updated_chunk = []\n",
    "    for gene in chunk:\n",
    "        updated_gene = update_gene_symbol(gene)\n",
    "        updated_chunk.append(updated_gene)\n",
    "    return updated_chunk\n",
    "\n",
    "print(\"Done. Scattering chunks to Dask workers...\")\n",
    "# Scatter the chunks to Dask workers\n",
    "scattered_chunks = dask.compute(chunks)[0]\n",
    "\n",
    "print(\"Done. Applying update function to each chunk in parallel...\")\n",
    "start_time = time.time()\n",
    "# Apply the update function to each chunk in parallel using Dask\n",
    "results = dask.compute(*[dask.delayed(update_chunk)(chunk) for chunk in scattered_chunks])\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed Time: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "print(\"Done. Combining the updated chunks into a single datatable...\")\n",
    "# Combine the updated chunks into a single datatable\n",
    "updated_data_list = sum(results, [])\n",
    "updated_table = dt.Frame(updated_data_list)\n",
    "\n",
    "# Add the second column of the original datatable to the updated datatable\n",
    "updated_table = dt.cbind(updated_table, big_table[:, 1])\n",
    "\n",
    "%store updated_table\n",
    "print(big_table)\n",
    "print(updated_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c958682d",
   "metadata": {},
   "source": [
    "Data table has been converted to gene symbols. Adding table 7 to table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datatable as dt\n",
    "\n",
    "# Load updated_table and gene_reg_table\n",
    "%store -r updated_table\n",
    "%store -r gene_reg_table\n",
    "#updated_table.names = {'C0': 'GenePair'}\n",
    "\n",
    "# Create the initial big table as a dictionary\n",
    "big_table_dict_2 = defaultdict(list)\n",
    "\n",
    "# Merge and update weights for each table\n",
    "print(\"Adding updated_table...\")\n",
    "merge_and_update(updated_table, \"WeightList\", big_table_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafbc4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r gene_reg_table\n",
    "\n",
    "merge_and_update(gene_reg_table, \"Gene Regularatory Weight\", big_table_dict_2)\n",
    "\n",
    "print(\"Finished adding all tables. Converting dictionary to dataframe...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b33fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists for GenePair and WeightList columns\n",
    "gene_pairs = []\n",
    "weight_lists = []\n",
    "\n",
    "# Iterate over the dictionary and populate the lists\n",
    "for gene_pair, weight_list in big_table_dict_2.items():\n",
    "    gene_pairs.append(gene_pair)\n",
    "    weight_lists.append(weight_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b5d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the big table using the populated lists\n",
    "\n",
    "print(\"Converting weights to list if it isn't already one...\")\n",
    "weight_lists = [list(w) if isinstance(w, (tuple, list)) else [w] for w in weight_lists]\n",
    "\n",
    "print(\"Done. Merging tables...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c59fec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene pairs and weight lists have been stored in the CSV file: gene_data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file = \"gene_data.csv\"\n",
    "\n",
    "# Write the data to the CSV file\n",
    "with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"GenePair\", \"WeightList\"])  # Write header\n",
    "\n",
    "    # Write the data rows\n",
    "    for gene_pair, weight_list in zip(gene_pairs, weight_lists):\n",
    "        writer.writerow([gene_pair, weight_list])\n",
    "\n",
    "print(\"Gene pairs and weight lists have been stored in the CSV file:\", csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f65f2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene pairs and weight lists have been stored in the txt file: gene_data_text.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the txt file path\n",
    "txt_file = \"gene_data_text.txt\"\n",
    "\n",
    "# Write the data to the txt file\n",
    "with open(txt_file, mode=\"w\") as file:\n",
    "    file.write(\"GenePair\\tWeightList\\n\")  # Write header\n",
    "\n",
    "    # Write the data rows\n",
    "    for gene_pair, weight_list in zip(gene_pairs, weight_lists):\n",
    "        file.write(f\"{gene_pair}\\t{weight_list}\\n\")\n",
    "\n",
    "print(\"Gene pairs and weight lists have been stored in the txt file:\", txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e63e2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from datatable import dt, update\n",
    "import dask.array as da\n",
    "import time\n",
    "\n",
    "def convert_to_gene_symbols(dictionary, table):\n",
    "\n",
    "    print(\"Converting datatable to a list of lists...\")\n",
    "    # Convert the datatable to a list of lists\n",
    "    data_list = table[:, 0].to_list()\n",
    "\n",
    "    print(\"Done. Splitting data list into chunks...\")\n",
    "    # Split the data list into chunks\n",
    "    chunks = dask.delayed(data_list)\n",
    "\n",
    "\n",
    "    # Define the function to update the gene symbols\n",
    "    def update_gene_symbol(gene):\n",
    "        gene1, gene2 = gene.split(',')[0], gene.split(',')[1]\n",
    "        output = gene_to_symbol.get(gene1, gene1) + \",\" + gene_to_symbol.get(gene2, gene2)\n",
    "        return output\n",
    "\n",
    "    # Define the function to update a chunk\n",
    "    def update_chunk(chunk):\n",
    "        updated_chunk = []\n",
    "        for gene in chunk:\n",
    "            updated_gene = update_gene_symbol(gene)\n",
    "            updated_chunk.append(updated_gene)\n",
    "        return updated_chunk\n",
    "\n",
    "    print(\"Done. Scattering chunks to Dask workers...\")\n",
    "    # Scatter the chunks to Dask workers\n",
    "    scattered_chunks = dask.compute(chunks)[0]\n",
    "\n",
    "    print(\"Done. Applying update function to each chunk in parallel...\")\n",
    "    start_time = time.time()\n",
    "    # Apply the update function to each chunk in parallel using Dask\n",
    "    results = dask.compute(*[dask.delayed(update_chunk)(chunk) for chunk in scattered_chunks])\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed Time: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "    print(\"Done. Combining the updated chunks into a single datatable...\")\n",
    "    # Combine the updated chunks into a single datatable\n",
    "    updated_data_list = sum(results, [])\n",
    "    updated_table = dt.Frame(updated_data_list)\n",
    "\n",
    "    # Add the second column of the original datatable to the updated datatable\n",
    "    updated_table = dt.cbind(updated_table, table[:, 1])\n",
    "    return updated_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f365e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
